# üïµÔ∏è Project: Real Time Ai Document Search Rag

> **Status:** Active Simulation  
> **Role:** Engineering Intern / Junior Dev

## üìã Mission Briefing
You have been onboarded to this project. Your Manager has set up the initial infrastructure and datasets.
**Your Goal:** Complete the Active Tickets assigned to you.

## üìÇ Project Structure
The DevOps team has pre-loaded these files for you:
- `docker-compose.yml`
- `main.py`
- `worker.py`
- `requirements.txt`
- `.env.example`

## üéüÔ∏è Active Tickets
Please implement the code to satisfy these requirements:

### Ticket #1: Implement Asynchronous Document Ingestion for Vector DB
Our current document ingestion process into the vector database (ChromaDB) is synchronous, blocking the main API and causing significant delays (up to several hours) before new information is searchable. Implement a Celery-based asynchronous worker system. This system should be responsible for: receiving new document content, chunking it, generating embeddings via the OpenAI API, and ingesting these embeddings into ChromaDB. Ensure that the ingestion process is resilient to failures and idempotent, allowing for safe retries without duplicating data.

### Ticket #2: Implement Redis Caching for OpenAI Embedding API Calls
Frequent calls to the OpenAI Embedding API are a major bottleneck in terms of latency and cost, especially during document ingestion and re-ingestion. Implement a Redis-backed caching layer for embedding generation. Before making an API call to OpenAI for a given text chunk, the system should first check if the corresponding embedding is already present in Redis. If found, use the cached embedding; otherwise, call OpenAI, store the result, and then use it. Consider a suitable cache expiration policy.

### Ticket #3: Optimize RAG Search API Latency with Result Caching
The RAG-powered search API can experience high latency under moderate to heavy load, particularly for frequently requested queries or complex retrievals. Implement a Redis-backed cache for storing recent search query results. This cache should significantly reduce response times for repeated queries. Design a robust cache invalidation strategy to ensure that search results remain fresh when underlying documents are updated or deleted, paying close attention to potential race conditions that could lead to stale data being served during concurrent updates and cache invalidations.

### Ticket #4: Dockerize the RAG System for Local Development and Deployment
The current setup for the RAG system is challenging to deploy consistently across environments. Dockerize the entire application stack using `docker-compose.yml`. This should include: the FastAPI application, Celery workers, Redis (for broker and caching), and ChromaDB (ensuring persistent storage for the vector database). Create a Multi-Stage Dockerfile for the application service to optimize image size and build times. The `docker-compose.yml` should allow for easy setup and linking of all services for a consistent local development and staging environment.


## üöÄ How to Submit
1. Clone this repository.
2. Write your code.
3. Push your changes to `main`.
4. Go back to the **Shadow Workplace Dashboard** and click **"Submit for Review"**.

---
*Generated by Shadow Workplace AI*
